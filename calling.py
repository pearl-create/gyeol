

import streamlit as st
import numpy as np
import av
from streamlit_webrtc import webrtc_streamer, WebRtcMode, AudioProcessorBase
import pathlib

# ---------------------
# 1. ë ˆì´ë¸” ë¶ˆëŸ¬ì˜¤ê¸°
# ---------------------
def load_labels(path: str):
    labels = {}
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            idx, name = line.strip().split(" ", 1)
            labels[int(idx)] = name
    return labels

LABELS = load_labels("labels.txt")  # 0~3 -> í•œê¸€ ë ˆì´ë¸” ë§µ
BAD_LABEL = "ì•„ì´ì”¨"  # ì¡ìœ¼ë©´ ê²½ê³  ë„ìš¸ ë‹¨ì–´

# ---------------------
# 2. TFLite ëª¨ë¸ ë¡œë”
# ---------------------
# tflite-runtime ì´ ìˆìœ¼ë©´ ê·¸ê±¸ ì“°ê³ , ì—†ìœ¼ë©´ tensorflow.lite ë¡œ ë–¨ì–´ì§€ê²Œ
try:
    from tflite_runtime.interpreter import Interpreter
except ImportError:
    from tensorflow.lite import Interpreter


@st.cache_resource
def load_model():
    model_path = pathlib.Path("soundclassifier_with_metadata.tflite")
    interpreter = Interpreter(model_path=str(model_path))
    interpreter.allocate_tensors()
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    return interpreter, input_details, output_details


interpreter, input_details, output_details = load_model()


# ---------------------
# 3. ì˜¤ë””ì˜¤ â†’ ëª¨ë¸ ì¶”ë¡  í•¨ìˆ˜
#    (ì—¬ê¸°ì„œëŠ” ì•„ì£¼ ë‹¨ìˆœí•˜ê²Œ: ë“¤ì–´ì˜¨ ë²„í¼ë¥¼ ì˜ë¼ì„œ ì²«ë²ˆì§¸ ì±„ë„ë§Œ ì”€)
# ---------------------
def classify_audio(audio_chunk: np.ndarray):
    """
    audio_chunk: np.float32, mono, [samples]
    ëª¨ë¸ì´ ìš”êµ¬í•˜ëŠ” shapeì— ë§ì¶° ë„£ì–´ì¤˜ì•¼ í•¨.
    Teachable Machine ì˜¤ë””ì˜¤ ëª¨ë¸ì€ ë³´í†µ [1, N] float32 ê¸°ëŒ€í•¨.
    """
    # 1) ëª¨ë¸ ì…ë ¥ shape í™•ì¸
    input_shape = input_details[0]["shape"]  # e.g. [1, 15600]
    required_len = input_shape[1]

    # 2) ê¸¸ì´ ë§ì¶”ê¸° (ë¶€ì¡±í•˜ë©´ 0íŒ¨ë”©, ê¸¸ë©´ ìë¥´ê¸°)
    if audio_chunk.shape[0] < required_len:
        padded = np.zeros(required_len, dtype=np.float32)
        padded[: audio_chunk.shape[0]] = audio_chunk
        audio_chunk = padded
    else:
        audio_chunk = audio_chunk[:required_len]

    # 3) ë°°ì¹˜ ì°¨ì› ë¶™ì´ê¸°
    audio_chunk = np.expand_dims(audio_chunk, axis=0).astype(np.float32)

    # 4) ì¸í„°í”„ë¦¬í„°ì— ë„£ê¸°
    interpreter.set_tensor(input_details[0]["index"], audio_chunk)
    interpreter.invoke()
    output_data = interpreter.get_tensor(output_details[0]["index"])
    # output_data shape: [1, num_classes]
    scores = output_data[0]
    pred_idx = int(np.argmax(scores))
    pred_label = LABELS.get(pred_idx, f"class_{pred_idx}")
    pred_score = float(scores[pred_idx])
    return pred_label, pred_score


# ---------------------
# 4. WebRTC ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì„œ
# ---------------------
class AudioProcessor(AudioProcessorBase):
    def __init__(self) -> None:
        self.last_label = None
        self.last_score = 0.0

    def recv_audio(self, frame: av.AudioFrame) -> av.AudioFrame:
        # av.AudioFrame -> np.ndarray (samples, channels)
        pcm = frame.to_ndarray()
        # stereoì¼ ìˆ˜ë„ ìˆìœ¼ë‹ˆê¹Œ ì²« ë²ˆì§¸ ì±„ë„ë§Œ ì‚¬ìš©
        if pcm.ndim == 2:
            pcm = pcm[:, 0]
        pcm = pcm.astype(np.float32)

        # 16bit PCMìœ¼ë¡œ ë“¤ì–´ì˜¤ë©´ scale í•„ìš”í•  ìˆ˜ë„ ìˆìŒ
        # ì—¬ê¸°ì„œëŠ” ëŒ€ì¶© ì •ê·œí™”
        if pcm.max() > 0:
            pcm = pcm / np.abs(pcm).max()

        label, score = classify_audio(pcm)
        self.last_label = label
        self.last_score = score

        # ì—¬ê¸°ì„œ ë°”ë¡œ ìŒì†Œê±° í•˜ê³  ì‹¶ìœ¼ë©´
        # return av.AudioFrame.from_ndarray(np.zeros_like(frame.to_ndarray()), layout=frame.layout)
        return frame


st.set_page_config(page_title="ìŒì„± ìš•ì„¤ ê°ì§€ ë°ëª¨", page_icon="ğŸ¤")

st.title("ğŸ¤ Teachable Machine ìŒì„± ë¶„ë¥˜ + Streamlit WebRTC ë°ëª¨")
st.write("ë§ˆì´í¬ í—ˆìš©ì„ ëˆŒëŸ¬ì„œ ì†Œë¦¬ë¥¼ ë‚´ë³´ë©´, ì•„ë˜ì— ì˜ˆì¸¡ ë¼ë²¨ì´ ë‚˜ì˜µë‹ˆë‹¤.")
st.write("ëª¨ë¸ ë¼ë²¨: 0 ê¹”ë¼ë§Œì”¨ / 1 ë°°ê²½ ì†ŒìŒ / 2 ìˆ˜ë°•ì”¨ / 3 ì•„ì´ì”¨")  # :contentReference[oaicite:3]{index=3}

webrtc_ctx = webrtc_streamer(
    key="audio-demo",
    mode=WebRtcMode.SENDONLY,  # ë‚´ ë§ˆì´í¬ë§Œ ë³´ë‚´ëŠ” ëª¨ë“œ
    audio_processor_factory=AudioProcessor,
    media_stream_constraints={"audio": True, "video": False},
    async_processing=True,
)

# í™”ë©´ì— ì˜ˆì¸¡ê°’ ë„ìš°ê¸°
if webrtc_ctx and webrtc_ctx.audio_processor:
    label = webrtc_ctx.audio_processor.last_label
    score = webrtc_ctx.audio_processor.last_score
    if label:
        st.metric("í˜„ì¬ ê°ì§€ëœ ë¼ë²¨", f"{label}", f"{score*100:.1f}%")
        if label == BAD_LABEL:
            st.error("ğŸš¨ ë¹„ì†ì–´(ì•„ì´ì”¨) ê°ì§€! í†µí™” ì œí•œ/ê²½ê³  ì²˜ë¦¬í•˜ì„¸ìš”.")
else:
    st.info("ë§ˆì´í¬ ì—°ê²°ì„ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘ì…ë‹ˆë‹¤.")
