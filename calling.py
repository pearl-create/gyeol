import streamlit as st
import numpy as np
import io
import time
from tflite_runtime.interpreter import Interpreter
from streamlit_webrtc import webrtc_streamer, WebRtcMode, AudioProcessorBase

# --- 1. ì„¤ì • ë° ëª¨ë¸ ë¡œë“œ ---

# íŒŒì¼ëª…ì€ ì‚¬ìš©ìê°€ ì—…ë¡œë“œí•œ íŒŒì¼ê³¼ ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
MODEL_PATH = "soundclassifier_with_metadata.tflite"
LABELS_PATH = "labels.txt"

# í‹°ì²˜ë¸” ë¨¸ì‹  ì˜¤ë””ì˜¤ ëª¨ë¸ì˜ ì…ë ¥ ìš”êµ¬ì‚¬í•­ (ë³´í†µ 1ì´ˆ ì˜¤ë””ì˜¤, 44100Hz)
TARGET_SAMPLE_RATE = 44100
AUDIO_CHUNK_SIZE = 1 * TARGET_SAMPLE_RATE # 1ì´ˆ ë¶„ëŸ‰ì˜ ìƒ˜í”Œ

@st.cache_resource
def load_teachable_machine_model():
    """TFLite ëª¨ë¸ê³¼ ë¼ë²¨ì„ ë¡œë“œí•˜ê³  ìºì‹±í•©ë‹ˆë‹¤."""
    try:
        # TFLite Interpreter ë¡œë“œ
        interpreter = Interpreter(model_path=MODEL_PATH)
        interpreter.allocate_tensors()
        
        # ëª¨ë¸ ì…ë ¥/ì¶œë ¥ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        input_details = interpreter.get_input_details()
        output_details = interpreter.get_output_details()
        
        # ë¼ë²¨ ë¡œë“œ
        with open(LABELS_PATH, 'r', encoding='utf-8') as f:
            # "0 í´ë˜ìŠ¤ëª…", "1 í´ë˜ìŠ¤ëª…" í˜•íƒœì—ì„œ í´ë˜ìŠ¤ëª…ë§Œ ì¶”ì¶œ
            labels = [line.strip().split(' ', 1)[1] for line in f.readlines()]
            
        return interpreter, labels, input_details, output_details
    except Exception as e:
        st.error(f"ëª¨ë¸ ë˜ëŠ” ë¼ë²¨ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}")
        st.error(f"ì˜¤ë¥˜ ìƒì„¸: {e}")
        st.warning("TFLite ëª¨ë¸ ì‹¤í–‰ í™˜ê²½(tflite_runtime)ì´ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return None, None, None, None

INTERPRETER, LABELS, INPUT_DETAILS, OUTPUT_DETAILS = load_teachable_machine_model()

# --- 2. ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ì²˜ë¦¬ í´ë˜ìŠ¤ ---

class TFLiteAudioProcessor(AudioProcessorBase):
    """
    streamlit-webrtcë¥¼ í†µí•´ ë“¤ì–´ì˜¤ëŠ” ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ TFLite ëª¨ë¸ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    def __init__(self, interpreter, input_details, output_details, labels):
        self.interpreter = interpreter
        self.input_details = input_details
        self.output_details = output_details
        self.labels = labels
        self.audio_buffer = np.array([], dtype=np.float32)
        
        # Streamlit UI ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ ì„¸ì…˜ ìƒíƒœ
        st.session_state.current_result = "ìŠ¤íŠ¸ë¦¼ ì‹œì‘ ëŒ€ê¸° ì¤‘..."
        st.session_state.detection_history = []

    def recv(self, frame):
        """WebRTC ì˜¤ë””ì˜¤ í”„ë ˆì„ì´ ë“¤ì–´ì˜¬ ë•Œë§ˆë‹¤ í˜¸ì¶œë©ë‹ˆë‹¤."""
        # 1. ì˜¤ë””ì˜¤ ë°ì´í„° ì¶”ì¶œ ë° float32ë¡œ ë³€í™˜
        # frame.to_ndarray()ëŠ” int16 í˜•ì‹ì˜ ì˜¤ë””ì˜¤ ë°°ì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        audio_int16 = np.frombuffer(frame.to_ndarray(), dtype=np.int16)
        audio_float32 = audio_int16.astype(np.float32) / 32768.0

        # ë²„í¼ì— ì˜¤ë””ì˜¤ ë°ì´í„° ì¶”ê°€
        self.audio_buffer = np.concatenate([self.audio_buffer, audio_float32])

        # 2. 1ì´ˆ(44100 ìƒ˜í”Œ) ë¶„ëŸ‰ì˜ ë°ì´í„°ê°€ ëª¨ì˜€ëŠ”ì§€ í™•ì¸
        if self.audio_buffer.size >= AUDIO_CHUNK_SIZE:
            
            # ëª¨ë¸ì— ì…ë ¥í•  1ì´ˆ ë°ì´í„° ì¶”ì¶œ
            input_data = self.audio_buffer[:AUDIO_CHUNK_SIZE]
            
            # ë²„í¼ì—ì„œ ì‚¬ìš©ëœ ë°ì´í„° ì œê±° (ë‚˜ë¨¸ì§€ ë°ì´í„°ëŠ” ë‹¤ìŒ í”„ë ˆì„ê³¼ í•©ì³ì§)
            self.audio_buffer = self.audio_buffer[AUDIO_CHUNK_SIZE:]
            
            # í‹°ì²˜ë¸” ë¨¸ì‹  TFLite ëª¨ë¸ì€ (1, 44100) í˜•íƒœì˜ ì…ë ¥ì„ ê¸°ëŒ€í•©ë‹ˆë‹¤.
            input_tensor = input_data.reshape(1, AUDIO_CHUNK_SIZE)

            # 3. ëª¨ë¸ ì˜ˆì¸¡ ì‹¤í–‰
            self.interpreter.set_tensor(self.input_details[0]['index'], input_tensor)
            self.interpreter.invoke()
            predictions = self.interpreter.get_tensor(self.output_details[0]['index'])[0]

            # 4. ê²°ê³¼ í•´ì„
            predicted_class_index = np.argmax(predictions)
            confidence = predictions[predicted_class_index]
            predicted_label = self.labels[predicted_class_index]
            
            # 5. ë¹„ì†ì–´ ê°ì§€ ë° ê¸°ë¡ (ë¼ë²¨ ì´ë¦„ì— ë”°ë¼ ì¡°ê±´ ìˆ˜ì •)
            is_profanity = any(word in predicted_label for word in ["ì•„ì´ì”¨", "ìš•ì„¤_ë¼ë²¨", "ë¹„ì†ì–´_ë¼ë²¨"]) # labels.txt ê¸°ë°˜ìœ¼ë¡œ ìˆ˜ì •
            
            if is_profanity and confidence > 0.7:
                detection_result = f"ğŸš¨ ë¹„ì†ì–´ ê°ì§€ë¨! ({predicted_label}: {confidence*100:.1f}%)"
            else:
                detection_result = f"âœ… ì •ìƒ ìŒì„± ê°ì§€ ì¤‘... ({predicted_label}: {confidence*100:.1f}%)"

            # 6. Streamlit UI ì—…ë°ì´íŠ¸ (ì„¸ì…˜ ìƒíƒœ ì‚¬ìš©)
            st.session_state.current_result = detection_result
            
            # ì´ë ¥ ì—…ë°ì´íŠ¸
            history = st.session_state.get('detection_history', [])
            history.append(detection_result)
            st.session_state.detection_history = history[-5:] # ìµœì‹  5ê°œë§Œ ìœ ì§€

        # ì˜¤ë””ì˜¤ í”„ë ˆì„ì€ ê·¸ëŒ€ë¡œ ë°˜í™˜
        return frame

# --- 3. Streamlit UI êµ¬ì„± ---

st.title("ğŸ—£ï¸ AI ë¹„ì†ì–´ ìŒì„± ê°ì§€ ì‹œìŠ¤í…œ (Streamlit + TFLite)")
st.markdown("---")

if INTERPRETER is None:
    st.warning("ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í•˜ì—¬ ê°ì§€ê¸°ë¥¼ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
else:
    st.info("ğŸ¤ ì•„ë˜ ë²„íŠ¼ì„ í´ë¦­í•˜ê³  ë§ˆì´í¬ ì ‘ê·¼ì„ í—ˆìš©í•˜ë©´ ì‹¤ì‹œê°„ ìŒì„± ê°ì§€ê°€ ì‹œì‘ë©ë‹ˆë‹¤.")

    # webrtc_streamer ì„¤ì • ë° ì‹¤í–‰
    webrtc_ctx = webrtc_streamer(
        key="audio-detector",
        mode=WebRtcMode.SENDONLY,
        # AudioProcessorBase í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ì—¬ ì˜¤ë””ì˜¤ ì²˜ë¦¬ì— ì‚¬ìš©
        audio_processor_factory=lambda: TFLiteAudioProcessor(
            INTERPRETER, INPUT_DETAILS, OUTPUT_DETAILS, LABELS
        ),
        # ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ë§Œ í•„ìš”í•˜ë¯€ë¡œ ë¹„ë””ì˜¤ëŠ” ë¹„í™œì„±í™”
        media_stream_constraints={"video": False, "audio": True}, 
    )

    # --- 4. ì‹¤ì‹œê°„ ê²°ê³¼ í‘œì‹œ ---
    st.subheader("ì‹¤ì‹œê°„ ê°ì§€ ê²°ê³¼")
    
    # UIëŠ” ë§¤ë²ˆ ë¦¬ëŸ°ë˜ë¯€ë¡œ ì„¸ì…˜ ìƒíƒœì˜ ìµœì‹  ê²°ê³¼ë¥¼ ê°€ì ¸ì™€ í‘œì‹œí•©ë‹ˆë‹¤.
    current_result = st.session_state.get('current_result', 'ìŠ¤íŠ¸ë¦¼ ì‹œì‘ ëŒ€ê¸° ì¤‘...')
    
    if "ğŸš¨ ë¹„ì†ì–´" in current_result:
        st.error(current_result)
    elif "âœ… ì •ìƒ" in current_result:
        st.success(current_result)
    else:
        st.text(current_result)
        
    st.subheader("ê°ì§€ ì´ë ¥")
    detection_history = st.session_state.get('detection_history', [])
    if detection_history:
        for i, result in enumerate(reversed(detection_history)):
            st.text(f"{len(detection_history) - i}: {result}")
