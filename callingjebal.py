# app.py (í•µì‹¬ ë¶€ë¶„ ìˆ˜ì •)
from tflite_support.task import audio
from tflite_support.task import core

# ëª¨ë¸ ë° ë¼ë²¨ íŒŒì¼ ê²½ë¡œ (GitHub ì €ì¥ì†Œì˜ ë£¨íŠ¸ ê²½ë¡œì— ìˆë‹¤ê³  ê°€ì •)
TFLITE_MODEL_PATH = "soundclassifier_with_metadata.tflite"
LABELS_PATH = "labels.txt" # ì´ íŒŒì¼ì—ëŠ” '0 ê¹”ë¼ë§Œì”¨\n1 ë°°ê²½ ì†ŒìŒ\n...' ì™€ ê°™ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

# ëª¨ë¸ ë¡œë“œ ë° ë¶„ë¥˜ê¸° ì´ˆê¸°í™”
@st.cache_resource
def load_classifier(model_path):
    # TFLite Task Libraryì˜ AudioClassifierë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ë¡œë“œ
    base_options = core.BaseOptions(file_name=model_path)
    options = audio.AudioClassifierOptions(base_options=base_options, max_results=4)
    classifier = audio.AudioClassifier.create_from_options(options)
    
    # ëª¨ë¸ì˜ ìƒ˜í”Œë§ ë ˆì´íŠ¸ì™€ ë²„í¼ í¬ê¸° ë“±ì„ ë©”íƒ€ë°ì´í„°ì—ì„œ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # classifier.get_required_sample_rate()
    # classifier.get_required_input_buffer_size()
    
    return classifier

# TFLiteAudioProcessor í´ë˜ìŠ¤ ìˆ˜ì •
class TFLiteAudioProcessor(AudioProcessorBase):
    def __init__(self, classifier):
        self.classifier = classifier
        # ëª¨ë¸ì´ ìš”êµ¬í•˜ëŠ” ê¸¸ì´ë§Œí¼ ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ëª¨ìœ¼ê¸° ìœ„í•œ ë²„í¼
        self.audio_record = audio.AudioData.create_from_array(
            np.zeros(classifier.get_required_input_buffer_size()), 
            classifier.get_required_sample_rate()
        )
        self.result = "ëŒ€ê¸° ì¤‘..."

    def recv(self, frame):
        # 1. WebRTC í”„ë ˆì„ì„ numpy ë°°ì—´ë¡œ ë³€í™˜
        audio_array = frame.to_ndarray(format="s16le")
        
        # 2. ë²„í¼ì— í˜„ì¬ í”„ë ˆì„ì˜ ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ì¶”ê°€ (tflite-support í™œìš©)
        # WebRTC ìŠ¤íŠ¸ë¦¼ì˜ ìƒ˜í”Œ ë ˆì´íŠ¸ê°€ ëª¨ë¸ê³¼ ë‹¤ë¥´ë©´ ë¦¬ìƒ˜í”Œë§ì´ í•„ìš”í•©ë‹ˆë‹¤.
        # (AudioData í´ë˜ìŠ¤ê°€ ë‚´ë¶€ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ì½”ë“œë¥¼ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤)

        # ì‹¤ì œ ë¡œì§: WebRTCì˜ ì˜¤ë””ì˜¤ í”„ë ˆì„ì„ AudioData ê°ì²´ì— ê³„ì† ì¶”ê°€í•©ë‹ˆë‹¤.
        # ì´ ë¶€ë¶„ì´ ê°€ì¥ ê¹Œë‹¤ë¡œìš°ë¯€ë¡œ, ê³µì‹ ì˜ˆì‹œë¥¼ ì°¸ê³ í•˜ì—¬ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.
        
        # ì„ì˜ì˜ ì¶”ë¡  ê²°ê³¼ ì—…ë°ì´íŠ¸ (ì‹¤ì œ ì½”ë“œì™€ ëŒ€ì²´ë˜ì–´ì•¼ í•¨)
        if np.random.rand() > 0.8:
            classification_result = self.classifier.classify(self.audio_record)
            
            top_category = classification_result.classifications[0].categories[0]
            self.result = f"{top_category.category_name} ({top_category.score:.2f})"
        
        return frame
    
# main í•¨ìˆ˜ ìˆ˜ì •
def main():
    st.title("ğŸ¤ ì‹¤ì‹œê°„ ìŒì„± ë¶„ë¥˜ê¸° (Streamlit + WebRTC)")
    
    # ë¶„ë¥˜ê¸° ë¡œë“œ
    classifier = load_classifier(TFLITE_MODEL_PATH)
    st.sidebar.success("âœ… TFLite ë¶„ë¥˜ê¸° ë¡œë“œ ì™„ë£Œ")
    
    # webrtc_streamer ì»´í¬ë„ŒíŠ¸ ì‹¤í–‰
    webrtc_ctx = webrtc_streamer(
        key="sound-classifier",
        mode=WebRtcMode.SENDONLY,
        audio_processor_factory=lambda: TFLiteAudioProcessor(classifier),
        media_stream_constraints={"video": False, "audio": True}
    )

    if webrtc_ctx.state.playing and webrtc_ctx.audio_processor:
        st.success("ğŸŸ¢ ë§ˆì´í¬ í™œì„±í™”ë¨: ë§í•´ë³´ì„¸ìš”!")
        st.write(f"í˜„ì¬ ë¶„ë¥˜ ê²°ê³¼: **{webrtc_ctx.audio_processor.result}**")
